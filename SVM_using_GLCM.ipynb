{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "from sklearn import svm, metrics, datasets\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import math\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import mahotas as mt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(img):\n",
    "    return mt.features.haralick(img)\n",
    "def load_image_files(container_path, dimension=(64, 64)):\n",
    "    \"\"\"\n",
    "    Load image files with categories as subfolder names \n",
    "    which performs like scikit-learn sample dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    container_path : string or unicode\n",
    "        Path to the main folder holding one subfolder per category\n",
    "    dimension : tuple\n",
    "        size to which image are adjusted to\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Bunch\n",
    "    \"\"\"\n",
    "    image_dir = Path(container_path)\n",
    "    folders = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n",
    "    categories = [fo.name for fo in folders]\n",
    "\n",
    "    descr = \"A image classification dataset\"\n",
    "    \n",
    "#     max image numbers\n",
    "    max_images=1000\n",
    "    \n",
    "#     revised code\n",
    "#   get the total length\n",
    "    total_len=0\n",
    "    for direct in folders:\n",
    "        for n,f in enumerate(direct.iterdir()):\n",
    "            total_len+=1\n",
    "            if n>max_images:\n",
    "                break\n",
    "            \n",
    "    print('length: '+str(total_len))\n",
    "    flat_data=[]\n",
    "    target=np.zeros(total_len)\n",
    "    index=0\n",
    "    \n",
    "    for i, direc in enumerate(folders):\n",
    "        print('importing files from '+direc.name)\n",
    "        for n,file in enumerate(direc.iterdir()):\n",
    "            img = cv2.cvtColor(imread(file),cv2.COLOR_RGB2GRAY)\n",
    "            img_resized = resize(img, dimension, anti_aliasing=True, mode='reflect')\n",
    "            img_resized=img_resized.astype(np.int)\n",
    "            img_resized=getFeature(img_resized).flatten()\n",
    "#             flat_data[index]=(img_resized.flatten())\n",
    "            flat_data.append(img_resized)\n",
    "#             images.append(img_resized)\n",
    "            target[index]=i\n",
    "            index+=1\n",
    "            if n>max_images:\n",
    "                break\n",
    "            del img\n",
    "            del img_resized\n",
    "    flat_data=np.array(flat_data)\n",
    "# #     original code\n",
    "##     images = []\n",
    "#     flat_data = []\n",
    "#     target = []\n",
    "#     for i, direc in enumerate(folders):\n",
    "#         print('adding files to '+direc.name)\n",
    "#         for file in direc.iterdir():\n",
    "#             img = imread(file)\n",
    "#             img_resized = resize(img, dimension, anti_aliasing=True, mode='reflect')\n",
    "#             flat_data.insert(img_resized.flatten(),0) \n",
    "# #             images.append(img_resized)\n",
    "#             target.insert(i,0)\n",
    "            \n",
    "#     flat_data = np.array(flat_data)\n",
    "#     target = np.array(target)\n",
    "#     images = np.array(images)\n",
    "\n",
    "    return Bunch(data=flat_data,\n",
    "                 target=target,\n",
    "                 target_names=categories,\n",
    "#                  images=images,\n",
    "                 DESCR=descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 10020\n",
      "importing files from Felt\n",
      "importing files from Artificial_leather\n",
      "importing files from Cotton\n",
      "importing files from Chenille\n",
      "importing files from Blended\n",
      "importing files from Corduroy\n",
      "importing files from Denim\n",
      "importing files from Artificial_fur\n",
      "importing files from Acrylic\n",
      "importing files from Crepe\n"
     ]
    }
   ],
   "source": [
    "image_dataset = load_image_files(\"Fabrics_train_lowClass/\",(120,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for - \n",
      "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
      "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=0):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       292\n",
      "         1.0       0.00      0.00      0.00       287\n",
      "         2.0       0.00      0.00      0.00       297\n",
      "         3.0       0.46      0.05      0.09       325\n",
      "         4.0       0.10      1.00      0.18       285\n",
      "         5.0       0.00      0.00      0.00       301\n",
      "         6.0       0.00      0.00      0.00       292\n",
      "         7.0       0.00      0.00      0.00       292\n",
      "         8.0       0.00      0.00      0.00       330\n",
      "         9.0       0.67      0.03      0.06       305\n",
      "\n",
      "   micro avg       0.10      0.10      0.10      3006\n",
      "   macro avg       0.12      0.11      0.03      3006\n",
      "weighted avg       0.13      0.10      0.03      3006\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    image_dataset.data, image_dataset.target, test_size=0.3,random_state=109)\n",
    "# train data with parameter optimization\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, param_grid)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# report\n",
    "print(\"Classification report for - \\n{}:\\n{}\\n\".format(clf, metrics.classification_report(y_test, y_pred)))\n",
    "\n",
    "\n",
    "# # dumping model\n",
    "\n",
    "# import pickle\n",
    "# with open('./clf.pickle','wb') as f:\n",
    "#     pickle.dump(clf,f)\n",
    "# # with open('./clf.pickle','rb') as f:\n",
    "# #     clf2=pickle.load(f)\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
